{
  "top.model_name": "LLaMA3-8B-Chat",
  "top.finetuning_type": "lora",
  "top.adapter_path": [],
  "top.quantization_bit": "none",
  "top.template": "llama3",
  "top.rope_scaling": "none",
  "top.booster": "none",
  "train.training_stage": "Supervised Fine-Tuning",
  "train.dataset_dir": "data",
  "train.dataset": [
    "pychrono"
  ],
  "train.learning_rate": "5e-5",
  "train.num_train_epochs": "3.0",
  "train.max_grad_norm": "1.0",
  "train.max_samples": "100000",
  "train.compute_type": "fp16",
  "train.cutoff_len": 1024,
  "train.batch_size": 2,
  "train.gradient_accumulation_steps": 8,
  "train.val_size": 0,
  "train.lr_scheduler_type": "cosine",
  "train.logging_steps": 5,
  "train.save_steps": 100,
  "train.warmup_steps": 0,
  "train.neftune_alpha": 0,
  "train.optim": "adamw_torch",
  "train.resize_vocab": false,
  "train.packing": false,
  "train.upcast_layernorm": false,
  "train.use_llama_pro": false,
  "train.shift_attn": false,
  "train.report_to": false,
  "train.num_layer_trainable": 3,
  "train.name_module_trainable": "all",
  "train.lora_rank": 8,
  "train.lora_alpha": 16,
  "train.lora_dropout": 0,
  "train.loraplus_lr_ratio": 0,
  "train.create_new_adapter": false,
  "train.use_rslora": true,
  "train.use_dora": false,
  "train.lora_target": "",
  "train.additional_target": "",
  "train.dpo_beta": 0.1,
  "train.dpo_ftx": 0,
  "train.orpo_beta": 0.1,
  "train.reward_model": null,
  "train.use_galore": false,
  "train.galore_rank": 16,
  "train.galore_update_interval": 200,
  "train.galore_scale": 0.25,
  "train.galore_target": "all"
}